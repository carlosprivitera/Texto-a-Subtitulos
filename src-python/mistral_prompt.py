# src-python/mistral_prompt.py
import sys
import os

# ‚ö†Ô∏è Asegurarse de que la GPU no se use (opcional pero recomendado)
os.environ["CUDA_VISIBLE_DEVICES"] = ""

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

def main():
    if len(sys.argv) < 2:
        print("‚ùå Error: se espera el texto como argumento.")
        sys.exit(1)

    entrada = sys.argv[1]

    print("üß† Cargando modelo Mistral...", flush=True)
    modelo_id = "mistralai/Mistral-7B-Instruct-v0.2"
    tokenizer = AutoTokenizer.from_pretrained(modelo_id)

    # üîß Forzar carga del modelo en CPU
    model = AutoModelForCausalLM.from_pretrained(modelo_id, device_map={"": "cpu"})

    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

    print("ü§ñ Generando respuesta...", flush=True)
    #//Los par√°metros significan:
    #//max_new_tokens: n√∫mero m√°ximo de tokens a generar, el rango m√≠nimo es 1 y el m√°ximo es 4096.
    #//do_sample: si se debe muestrear de la distribuci√≥n de probabilidad
    #//temperature: controla la aleatoriedad de la generaci√≥n (0.7 es un valor com√∫n),
    #// valores m√°s bajos hacen que la salida sea m√°s predecible, mientras que valores m√°s altos la hacen m√°s creativa.
    #//Otros par√°metros como top_k y top_p tambi√©n se pueden ajustar para controlar la diversidad de la salida.
    #//top_k: n√∫mero de palabras m√°s probables a considerar en cada paso de generaci√≥n. Los valores m√°s bajos hacen que la salida sea m√°s predecible.
    #// el valor m√≠nimo es 1 y el m√°ximo es 100. El valor por defecto es 50.
    #//top_k: es una t√©cnica de muestreo que limita el n√∫mero de palabras candidatas a las m√°s probables,
    #// los valores m√°s altos permiten m√°s diversidad. El valor m√≠nimo es 1 y el m√°ximo es 100. El valor por defecto es 50. 
    respuesta = pipe(entrada, max_new_tokens=1024, do_sample=True, temperature=0.7)[0]["generated_text"]

    print("‚úÖ Respuesta generada:\n", flush=True)
    print(respuesta)

if __name__ == "__main__":
    main()
    
    
    