# src-python/mistral_prompt.py
import sys
import os

# ‚ö†Ô∏è Asegurarse de que la GPU no se use (opcional pero recomendado)
os.environ["CUDA_VISIBLE_DEVICES"] = ""

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

def main():
    if len(sys.argv) < 2:
        print("‚ùå Error: se espera el texto como argumento.")
        sys.exit(1)

    entrada = sys.argv[1]

    print("üß† Cargando modelo Mistral...", flush=True)
    modelo_id = "mistralai/Mistral-7B-Instruct-v0.2"
    tokenizer = AutoTokenizer.from_pretrained(modelo_id)

    # üîß Forzar carga del modelo en CPU
    model = AutoModelForCausalLM.from_pretrained(modelo_id, device_map={"": "cpu"})

    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

    print("ü§ñ Generando respuesta...", flush=True)
    #//Los par√°metros significan:
    #//max_new_tokens: n√∫mero m√°ximo de tokens a generar
    #//do_sample: si se debe muestrear de la distribuci√≥n de probabilidad
    #//temperature: controla la aleatoriedad de la generaci√≥n (0.7 es un valor com√∫n)
    #//Otros par√°metros como top_k y top_p tambi√©n se pueden ajustar para controlar la diversidad de la salida.
    # Generar respuesta
    respuesta = pipe(entrada, max_new_tokens=1024, do_sample=True, temperature=0.7)[0]["generated_text"]

    print("‚úÖ Respuesta generada:\n", flush=True)
    print(respuesta)

if __name__ == "__main__":
    main()
    
    
    